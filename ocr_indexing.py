#!/usr/bin/env python3
"""
åŸºäºOCR JSONæ–‡ä»¶çš„PageIndexç´¢å¼•ç”Ÿæˆ
ä½¿ç”¨é«˜è´¨é‡çš„OCRè¯†åˆ«ç»“æœè¿›è¡Œæ–‡æ¡£ç»“æ„åˆ†æ
æ”¯æŒå¤šæ¨¡å‹é…ç½®å’Œç»Ÿè®¡ä¿¡æ¯
"""

import os
import json
import time
import argparse
from pathlib import Path
import openai
from prompt_templates import prompt_manager
from model_manager import get_model_manager
import re

def get_model_config(model_id: str = None):
    """è·å–æ¨¡å‹é…ç½®"""
    manager = get_model_manager()
    if model_id:
        model_config = manager.get_model_config(model_id)
        if model_config:
            return {
                'api_key': model_config.api_key,
                'base_url': model_config.base_url,
                'model_name': model_config.model_name,
                'max_tokens': model_config.max_tokens,
                'context_limit': model_config.context_limit
            }
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ¨¡å‹é…ç½®ï¼ŒæŠ›å‡ºé”™è¯¯
    raise ValueError(f"æ¨¡å‹ {model_id} ä¸å­˜åœ¨æˆ–æœªå¯ç”¨")

def parse_ocr_json(ocr_file: str):
    """è§£æOCR JSONæ–‡ä»¶"""
    with open(ocr_file, 'r', encoding='utf-8') as f:
        content_list = json.load(f)
    
    # æŒ‰é¡µé¢ç»„ç»‡å†…å®¹
    pages = {}
    for item in content_list:
        page_idx = item.get('page_idx', 0)
        if page_idx not in pages:
            pages[page_idx] = {'texts': [], 'tables': []}
        
        if item['type'] == 'text':
            pages[page_idx]['texts'].append({
                'text': item['text'],
                'text_level': item.get('text_level', 0)
            })
        elif item['type'] == 'table':
            # è§£æHTMLè¡¨æ ¼
            table_html = item['table_body']
            table_data = parse_html_table(table_html)
            pages[page_idx]['tables'].append({
                'caption': item.get('table_caption', []),
                'footnote': item.get('table_footnote', []),
                'data': table_data,
                'img_path': item.get('img_path', '')
            })
    
    return pages

def parse_html_table(html_content: str):
    """è§£æHTMLè¡¨æ ¼å†…å®¹"""
    try:
        # ç®€å•çš„HTMLè¡¨æ ¼è§£æï¼Œæå–tdå’Œthæ ‡ç­¾å†…å®¹
        rows = []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–è¡¨æ ¼è¡Œ
        tr_pattern = r'<tr[^>]*>(.*?)</tr>'
        tr_matches = re.findall(tr_pattern, html_content, re.DOTALL)
        
        for tr_content in tr_matches:
            row = []
            # æå–tdå’Œthæ ‡ç­¾å†…å®¹
            cell_pattern = r'<(?:td|th)[^>]*>(.*?)</(?:td|th)>'
            cell_matches = re.findall(cell_pattern, tr_content, re.DOTALL)
            
            for cell_content in cell_matches:
                # æ¸…ç†HTMLæ ‡ç­¾
                clean_text = re.sub(r'<[^>]+>', '', cell_content).strip()
                row.append({
                    'text': clean_text,
                    'rowspan': 1,
                    'colspan': 1
                })
            
            if row:
                rows.append(row)
        
        return rows
    except Exception as e:
        print(f"âš ï¸ è¡¨æ ¼è§£æå¤±è´¥: {e}")
        return []

def extract_table_summary(table_data):
    """æå–è¡¨æ ¼æ‘˜è¦ä¿¡æ¯"""
    if not table_data:
        return "æ— è¡¨æ ¼æ•°æ®"
    
    # æå–è¡¨å¤´
    headers = []
    if table_data:
        for cell in table_data[0]:
            headers.append(cell['text'])
    
    # æå–å…³é”®æ•°æ®
    key_data = []
    for row in table_data[1:6]:  # åªå–å‰5è¡Œæ•°æ®
        row_data = []
        for cell in row:
            row_data.append(cell['text'])
        key_data.append(' | '.join(row_data))
    
    summary = f"è¡¨å¤´: {' | '.join(headers)}\n"
    if key_data:
        summary += f"æ•°æ®ç¤ºä¾‹: {'; '.join(key_data[:3])}"
    
    return summary

def create_ocr_structure_with_llm(pages, model_id, scenario="water_engineering"):
    """ä½¿ç”¨LLMåˆ†æOCRå†…å®¹åˆ›å»ºæ–‡æ¡£ç»“æ„"""
    
    total_pages = len(pages)
    all_structures = []
    
    # è·å–æ¨¡å‹é…ç½®
    manager = get_model_manager()
    model_config = manager.get_model_config(model_id)
    if not model_config:
        raise ValueError(f"æ¨¡å‹ {model_id} ä¸å­˜åœ¨æˆ–æœªå¯ç”¨")
    
    context_limit = model_config.context_limit
    max_tokens = model_config.max_tokens
    
    # ä¼°ç®—æ¯é¡µå†…å®¹é•¿åº¦ï¼ŒåŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°
    estimated_chars_per_page = 500  # æ¯é¡µå¤§çº¦500å­—ç¬¦
    batch_size = max(10, min(50, context_limit // (estimated_chars_per_page * 2)))  # ä¿å®ˆä¼°è®¡
    
    print(f"ğŸ”§ æ¨¡å‹é…ç½®: {model_config.name} ({model_config.model_name})")
    print(f"ğŸ“ ä¸Šä¸‹æ–‡é™åˆ¶: {context_limit} tokens")
    print(f"ğŸ“¤ æœ€å¤§è¾“å‡º: {max_tokens} tokens")
    print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size} é¡µ/æ‰¹æ¬¡")
    print(f"ğŸ”„ é¢„è®¡æ‰¹æ¬¡: {(total_pages + batch_size - 1) // batch_size} ä¸ª")
    
    for batch_start in range(0, total_pages, batch_size):
        batch_end = min(batch_start + batch_size, total_pages)
        print(f"ğŸ”„ å¤„ç†ç¬¬ {batch_start+1}-{batch_end} é¡µ...")
        
        # æ„å»ºå½“å‰æ‰¹æ¬¡çš„æ ·æœ¬å†…å®¹
        sample_content = ""
        max_content_length = int(context_limit * 0.6)  # ä½¿ç”¨60%çš„ä¸Šä¸‹æ–‡é™åˆ¶
        
        for i in range(batch_start, batch_end):
            if i not in pages:
                continue
                
            page_content = pages[i]
            page_header = f"=== ç¬¬{i+1}é¡µ ===\n"
            
            # æ£€æŸ¥æ˜¯å¦è¶…å‡ºé•¿åº¦é™åˆ¶
            if len(sample_content) + len(page_header) > max_content_length:
                print(f"âš ï¸ å†…å®¹é•¿åº¦é™åˆ¶ï¼Œæˆªæ–­åˆ°ç¬¬{i}é¡µ")
                break
                
            sample_content += page_header
            
            # æ·»åŠ æ–‡æœ¬å†…å®¹ï¼ˆåŠ¨æ€é™åˆ¶ï¼‰
            text_count = 0
            max_texts_per_page = max(2, min(5, (max_content_length - len(sample_content)) // 200))
            
            for text_item in page_content['texts']:
                if text_count >= max_texts_per_page:
                    break
                level = text_item.get('text_level', 0)
                # åŠ¨æ€è°ƒæ•´æ–‡æœ¬é•¿åº¦
                max_text_length = max(100, (max_content_length - len(sample_content)) // max_texts_per_page)
                text = text_item['text'][:max_text_length]
                if level > 0:
                    sample_content += f"[æ ‡é¢˜{level}] {text}\n"
                else:
                    sample_content += f"{text}\n"
                text_count += 1
            
            # æ·»åŠ è¡¨æ ¼å†…å®¹ï¼ˆåŠ¨æ€é™åˆ¶ï¼‰
            table_count = 0
            max_tables_per_page = max(1, min(3, (max_content_length - len(sample_content)) // 300))
            
            for table_item in page_content['tables']:
                if table_count >= max_tables_per_page:
                    break
                table_summary = extract_table_summary(table_item['data'])
                # é™åˆ¶è¡¨æ ¼æ‘˜è¦é•¿åº¦
                if len(table_summary) > 200:
                    table_summary = table_summary[:200] + "..."
                sample_content += f"[è¡¨æ ¼] {table_summary}\n"
                table_count += 1
            
            sample_content += "\n"
        
        # ä½¿ç”¨æç¤ºè¯æ¨¡æ¿
        prompt = prompt_manager.format_template(
            scenario,
            "structure_analysis",
            document_sample=sample_content
        )
    
        # è°ƒç”¨LLMåˆ†æå½“å‰æ‰¹æ¬¡
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # ä½¿ç”¨æ¨¡å‹ç®¡ç†å™¨è°ƒç”¨LLM
                messages = [{"role": "user", "content": prompt}]
                result = manager.call_model(model_id, messages, max_tokens=max_tokens)
                
                if not result['success']:
                    raise Exception(f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {result['error']}")
                
                result_text = result['content'].strip()
                print(f"ğŸ” LLMè¿”å›å†…å®¹: {result_text[:200]}...")
                print(f"â±ï¸ è°ƒç”¨è€—æ—¶: {result['elapsed_time']:.2f}ç§’")
                print(f"ğŸ”¢ Tokenä½¿ç”¨: {result['tokens_used']}")
                print(f"ğŸ“Š ç´¯è®¡è°ƒç”¨: {result['stats']['total_calls']}æ¬¡")
                
                # å°è¯•è§£æJSON
                structure = None
                
                # é¦–å…ˆå°è¯•ç›´æ¥è§£æ
                try:
                    result = json.loads(result_text)
                    structure = result.get('structure', [])
                except json.JSONDecodeError:
                    # å°è¯•å¤šç§æ¸…ç†æ–¹å¼
                    cleaned_text = result_text
                    
                    # ç§»é™¤markdownä»£ç å—æ ‡è®°
                    if "```json" in cleaned_text:
                        cleaned_text = cleaned_text.split("```json")[1].split("```")[0]
                    elif "```" in cleaned_text:
                        cleaned_text = cleaned_text.split("```")[1].split("```")[0]
                    
                    # ç§»é™¤å¯èƒ½çš„é¢å¤–æ–‡æœ¬
                    if "{" in cleaned_text and "}" in cleaned_text:
                        start = cleaned_text.find("{")
                        end = cleaned_text.rfind("}") + 1
                        cleaned_text = cleaned_text[start:end]
                    
                    try:
                        result = json.loads(cleaned_text.strip())
                        structure = result.get('structure', [])
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ JSONè§£æå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                        print(f"åŸå§‹è¿”å›å‰200å­—ç¬¦: {result_text[:200]}")
                        print(f"æ¸…ç†åå‰200å­—ç¬¦: {cleaned_text[:200]}")
                        
                        if attempt < max_retries - 1:
                            print("ğŸ”„ é‡è¯•ä¸­...")
                            continue
                        else:
                            print("âŒ å¤šæ¬¡å°è¯•åä»æ— æ³•è§£æJSONï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                            break
                
                if structure:
                    print(f"âœ… æ‰¹æ¬¡ {batch_start+1}-{batch_end} è¯†åˆ«å‡º {len(structure)} ä¸ªèŠ‚ç‚¹")
                    all_structures.extend(structure)
                    break
                else:
                    print("âš ï¸ LLMè¿”å›ç©ºç»“æ„ï¼Œé‡è¯•...")
                    continue
                    
            except Exception as e:
                print(f"âŒ LLMè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    print("ğŸ”„ é‡è¯•ä¸­...")
                    continue
                else:
                    print("âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                    break
    
    print(f"âœ… æ€»å…±è¯†åˆ«å‡º {len(all_structures)} ä¸ªèŠ‚ç‚¹")
    return all_structures

def generate_ocr_index(ocr_file: str, scenario: str = "water_engineering", model_id: str = None):
    """åŸºäºOCRæ–‡ä»¶ç”Ÿæˆç´¢å¼•"""
    
    # è·å–æ¨¡å‹é…ç½®
    manager = get_model_manager()
    if model_id:
        model_config = manager.get_model_config(model_id)
        if not model_config:
            print(f"âŒ æ¨¡å‹ {model_id} ä¸å­˜åœ¨æˆ–æœªå¯ç”¨")
            return None
    else:
        model_id = manager.get_default_model()
        model_config = manager.get_model_config(model_id)
    
    print(f"ğŸ”§ ä½¿ç”¨æ¨¡å‹: {model_config.name} ({model_config.model_name})")
    print(f"ğŸ“„ å¤„ç†OCRæ–‡ä»¶: {ocr_file}")
    print(f"ğŸ¯ åœºæ™¯: {prompt_manager.templates['scenarios'][scenario]['name']}")
    
    try:
        # è§£æOCRæ–‡ä»¶
        print("ğŸ”„ è§£æOCRæ–‡ä»¶...")
        pages = parse_ocr_json(ocr_file)
        print(f"ğŸ“„ è§£æåˆ° {len(pages)} é¡µ")
        
        # ç»Ÿè®¡è¡¨æ ¼æ•°é‡
        total_tables = sum(len(page['tables']) for page in pages.values())
        print(f"ğŸ“Š å‘ç° {total_tables} ä¸ªè¡¨æ ¼")
        
        # ä½¿ç”¨LLMåˆ›å»ºç»“æ„
        print("ğŸ”„ ä½¿ç”¨LLMåˆ†ææ–‡æ¡£ç»“æ„...")
        structure = create_ocr_structure_with_llm(pages, model_id, scenario)
        
        # ä¸ºæ¯ä¸ªèŠ‚ç‚¹æ·»åŠ IDå’Œé¡µé¢èŒƒå›´
        for i, node in enumerate(structure):
            node['node_id'] = f"{i:04d}"
            node['start_index'] = node.get('start_page', 1)
            node['end_index'] = node.get('end_page', 1)
        
        # ç”Ÿæˆç»“æœ
        stats = manager.get_stats(model_id)
        model_stats = stats.get(model_id)
        
        # å°†CallStatså¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
        stats_dict = {}
        if model_stats:
            stats_dict = {
                'total_calls': model_stats.total_calls,
                'success_calls': model_stats.success_calls,
                'failed_calls': model_stats.failed_calls,
                'total_time': model_stats.total_time,
                'total_tokens': model_stats.total_tokens
            }
        
        result = {
            'doc_name': Path(ocr_file).stem,
            'scenario': scenario,
            'model_id': model_id,
            'model_name': model_config.name,
            'total_pages': len(pages),
            'total_tables': total_tables,
            'structure': structure,
            'indexing_stats': stats_dict  # é‡å‘½åä¸ºindexing_statsï¼Œç”¨äºStreamlitæ˜¾ç¤º
        }
        
        # ä¿å­˜ç»“æœï¼ˆä¸åŒ…å«ç»Ÿè®¡ä¿¡æ¯ï¼‰
        manager.ensure_directories()
        index_dir = manager.get_directory("index_files")
        file_name = Path(ocr_file).stem
        output_file = os.path.join(index_dir, f"{file_name}_{scenario}_{model_id}_ocr_index.json")
        
        # åˆ›å»ºä¸åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„ç´¢å¼•æ–‡ä»¶
        index_data = {
            'doc_name': result['doc_name'],
            'scenario': result['scenario'],
            'model_id': result['model_id'],
            'model_name': result['model_name'],
            'total_pages': result['total_pages'],
            'total_tables': result['total_tables'],
            'structure': result['structure']
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ç´¢å¼•ç”Ÿæˆå®Œæˆ!")
        print(f"ğŸ“ ä¿å­˜ä½ç½®: {output_file}")
        print(f"ğŸ“„ æ–‡æ¡£åç§°: {result['doc_name']}")
        print(f"ğŸŒ³ èŠ‚ç‚¹æ•°: {len(result['structure'])}")
        print(f"ğŸ“Š è¡¨æ ¼æ•°: {result['total_tables']}")
        
        # è¿”å›ç»“æœå­—å…¸è€Œä¸æ˜¯æ–‡ä»¶è·¯å¾„
        result['output_file'] = output_file
        return result
        
    except Exception as e:
        print(f"âŒ ç´¢å¼•ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """ä¸»å‡½æ•°"""
    import os
    
    parser = argparse.ArgumentParser(description='åŸºäºOCRçš„PageIndexç´¢å¼•ç”Ÿæˆ')
    parser.add_argument('--ocr_file', type=str, required=True, help='OCR JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--scenario', type=str, default='water_engineering', 
                       help='åœºæ™¯ç±»å‹ (water_engineering, financial_report, technical_manual)')
    parser.add_argument('--model_id', type=str, help='æ¨¡å‹ID (ä»model_configs.yamlä¸­é€‰æ‹©)')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.ocr_file):
        print(f"âŒ OCRæ–‡ä»¶ä¸å­˜åœ¨: {args.ocr_file}")
        return
    
    # è·å–æ¨¡å‹ID
    manager = get_model_manager()
    if args.model_id:
        if args.model_id not in manager.models:
            print(f"âŒ æ¨¡å‹ {args.model_id} ä¸å­˜åœ¨æˆ–æœªå¯ç”¨")
            print(f"å¯ç”¨æ¨¡å‹: {list(manager.models.keys())}")
            return
        model_id = args.model_id
    else:
        model_id = manager.get_default_model()
        print(f"ğŸ”§ ä½¿ç”¨é»˜è®¤æ¨¡å‹: {model_id}")
    
    generate_ocr_index(args.ocr_file, args.scenario, model_id)

if __name__ == "__main__":
    main()
